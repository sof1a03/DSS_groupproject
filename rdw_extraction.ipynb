{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()   # then create the BigQuery client"
      ],
      "metadata": {
        "id": "lSPUR9C-E0FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MldqTuEz_Gn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import unicodedata\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.auth.exceptions import DefaultCredentialsError, RefreshError\n",
        "from google.oauth2 import service_account\n",
        "import concurrent.futures as _fut"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID   = os.getenv(\"PROJECT_ID\",   \"compact-garage-473209-u4\")\n",
        "DATASET_ID   = os.getenv(\"DATASET_ID\",   \"RDW_final\")\n",
        "LOCATION     = os.getenv(\"LOCATION\",     \"EU\")\n",
        "\n",
        "# Year window\n",
        "YEAR_START   = int(os.getenv(\"YEAR_START\", \"2023\"))\n",
        "YEAR_END     = int(os.getenv(\"YEAR_END\",   \"2025\"))\n",
        "\n",
        "# Final table name (only table we write)\n",
        "TBL_AGG      = os.getenv(\"TBL_AGG\", \"rdw_brand_model_peryear\")\n",
        "\n",
        "# RDW SODA config RDW db\n",
        "SODA_BASE     = \"https://opendata.rdw.nl/resource\"\n",
        "RESOURCE_VEH  = \"m9d7-ebf2\"   # Gekentekende voertuigen\n",
        "RESOURCE_FUEL = \"8ys7-d773\"   # Gekentekende voertuigen brandstof\n",
        "RDW_APP_TOKEN = os.getenv(\"RDW_APP_TOKEN\")  # optional\n",
        "\n",
        "# Fetch sizing\n",
        "PAGE_LIMIT = int(os.getenv(\"PAGE_LIMIT\", \"50000\"))\n",
        "MAX_ROWS   = os.getenv(\"MAX_ROWS\")\n",
        "MAX_ROWS   = int(MAX_ROWS) if MAX_ROWS else None\n",
        "\n",
        "# Google Programmable Search (Custom Search JSON API)\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyCsGoDVcx99QBb5Xd50BPCUFSEk0mN2XGA\")\n",
        "GOOGLE_CSE_ID  = os.getenv(\"GOOGLE_CSE_ID\",  \"26a5701f7452744ed\")\n",
        "IMG_VALIDATE   = os.getenv(\"IMG_VALIDATE\", \"true\").lower() == \"true\"  # HEAD/GET quick check\n",
        "IMG_FALLBACK_OV= os.getenv(\"IMG_FALLBACK_OPENVERSE\", \"true\").lower() == \"true\"\n",
        "IMG_SLEEP_S    = float(os.getenv(\"IMG_SLEEP_S\", \"0.2\")) #only for sequential version\n",
        "IMG_TOP_N      = os.getenv(\"IMG_TOP_N\")  # limit enrichment to top-N rows (by latest-year count) to save quota\n",
        "IMG_TOP_N      = int(IMG_TOP_N) if IMG_TOP_N else None\n",
        "\n",
        "SA_PATH = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\")\n",
        "SA_JSON = os.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\", \"\")"
      ],
      "metadata": {
        "id": "yIJIND5b0C-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bq_client(project_id: str) -> bigquery.Client:\n",
        "    try:\n",
        "        return bigquery.Client(project=project_id)\n",
        "    except (DefaultCredentialsError, RefreshError) as e_default:\n",
        "        print(\"[AUTH] Default credentials not usable:\", repr(e_default))\n",
        "\n",
        "    if SA_PATH and os.path.exists(SA_PATH):\n",
        "        print(f\"[AUTH] Using service account file at {SA_PATH}\")\n",
        "        creds = service_account.Credentials.from_service_account_file(SA_PATH)\n",
        "        return bigquery.Client(project=project_id, credentials=creds)\n",
        "\n",
        "    if SA_JSON:\n",
        "        try:\n",
        "            print(\"[AUTH] Using service account JSON from GOOGLE_SERVICE_ACCOUNT_JSON\")\n",
        "            info = json.loads(SA_JSON)\n",
        "            creds = service_account.Credentials.from_service_account_info(info)\n",
        "            return bigquery.Client(project=project_id, credentials=creds)\n",
        "        except Exception as e_json:\n",
        "            print(\"[AUTH] Failed to parse GOOGLE_SERVICE_ACCOUNT_JSON:\", repr(e_json))\n",
        "\n",
        "    try:\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "        print(\"[AUTH] Starting Colab OAuth flow...\")\n",
        "        colab_auth.authenticate_user()\n",
        "        print(\"[AUTH] Colab user authenticated; using ADC\")\n",
        "        return bigquery.Client(project=project_id)\n",
        "    except Exception as e_colab:\n",
        "        print(\"[AUTH] Colab OAuth not available / failed:\", repr(e_colab))\n",
        "        raise RuntimeError(\n",
        "            \"No valid Google Cloud credentials found. \"\n",
        "            \"Provide a service account or run in Colab and authenticate.\"\n",
        "        )"
      ],
      "metadata": {
        "id": "Pva_YL9W0Qzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### helpers functions"
      ],
      "metadata": {
        "id": "thKhdeLT1LJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soda_headers() -> Dict[str, str]:\n",
        "    h = {\"Accept\": \"application/json\", \"User-Agent\": \"rdw-bq-loader/3.1\"}\n",
        "    if RDW_APP_TOKEN:\n",
        "        h[\"X-App-Token\"] = RDW_APP_TOKEN\n",
        "    return h"
      ],
      "metadata": {
        "id": "GUMG2BCP0hcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_soda(\n",
        "    resource: str,\n",
        "    select: str,\n",
        "    where: Optional[str] = None,\n",
        "    order: Optional[str] = None,\n",
        "    limit: int = 50000,\n",
        "    max_rows: Optional[int] = None,\n",
        "    timeout: int = 120,\n",
        ") -> pd.DataFrame:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    offset = 0\n",
        "    while True:\n",
        "        params = {\"$select\": select, \"$limit\": limit, \"$offset\": offset}\n",
        "        if where: params[\"$where\"] = where\n",
        "        if order: params[\"$order\"] = order\n",
        "        url = f\"{SODA_BASE}/{resource}.json\"\n",
        "\n",
        "        r = requests.get(url, params=params, headers=soda_headers(), timeout=timeout)\n",
        "        if not r.ok:\n",
        "            print(\"\\n[SODA] ERROR URL:\", r.url, file=sys.stderr)\n",
        "            print(\"[SODA] ERROR BODY]:\", r.text[:1000], file=sys.stderr)\n",
        "            r.raise_for_status()\n",
        "\n",
        "        batch = r.json()\n",
        "        if not batch:\n",
        "            break\n",
        "        rows.extend(batch)\n",
        "\n",
        "        if max_rows and len(rows) >= max_rows:\n",
        "            rows = rows[:max_rows]\n",
        "            break\n",
        "\n",
        "        if len(batch) < limit:\n",
        "            break\n",
        "\n",
        "        offset += limit\n",
        "        time.sleep(0.05)  # be kind to API\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "AdBslYlm0jRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_dataset(client: bigquery.Client, dataset_id: str, location: str = \"EU\"):\n",
        "    ds_ref = f\"{client.project}.{dataset_id}\"\n",
        "    try:\n",
        "        client.get_dataset(ds_ref)\n",
        "        print(f\"[BQ] Dataset exists: {ds_ref}\")\n",
        "    except NotFound:\n",
        "        print(f\"[BQ] Creating dataset: {ds_ref} in {location} ...\")\n",
        "        ds = bigquery.Dataset(ds_ref)\n",
        "        ds.location = location\n",
        "        client.create_dataset(ds, exists_ok=True)\n",
        "        print(f\"[BQ] Dataset created: {ds_ref}\")"
      ],
      "metadata": {
        "id": "e2apsVTr0m41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_bigquery(\n",
        "    client: bigquery.Client,\n",
        "    rows: List[Dict[str, Any]],\n",
        "    table_id: str,\n",
        "    schema: List[bigquery.SchemaField],\n",
        "    write_disposition: str = \"WRITE_TRUNCATE\",\n",
        "    batch_size: int = 50000,\n",
        "):\n",
        "    if not rows:\n",
        "        print(f\"[BQ] No data to load for {table_id}\")\n",
        "        return\n",
        "\n",
        "    safe_rows: List[Dict[str, Any]] = []\n",
        "    bad = 0\n",
        "    for i, r in enumerate(rows):\n",
        "        try:\n",
        "            _ = json.loads(json.dumps(r, ensure_ascii=False))\n",
        "            safe_rows.append(r)\n",
        "        except Exception as e:\n",
        "            bad += 1\n",
        "            if bad <= 5:\n",
        "                print(f\"[SANITY] Dropping row {i} due to JSON error: {e}. Snippet: {str(r)[:200]}\", file=sys.stderr)\n",
        "    if bad:\n",
        "        print(f\"[SANITY] Dropped {bad} malformed rows before BigQuery load.\", file=sys.stderr)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=write_disposition)\n",
        "    for i in range(0, len(safe_rows), batch_size):\n",
        "        chunk = safe_rows[i:i + batch_size]\n",
        "        job = client.load_table_from_json(chunk, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "        print(f\"[BQ] Loaded {len(chunk)} rows into {table_id} (batch {i//batch_size+1})\")"
      ],
      "metadata": {
        "id": "F2c2OaXe0o0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDW fetchers (multi-year)"
      ],
      "metadata": {
        "id": "q0_OY6xD1R8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _norm_plate(s):\n",
        "    if pd.isna(s):\n",
        "        return s\n",
        "    return str(s).upper().replace(\" \", \"\").replace(\"-\", \"\")\n"
      ],
      "metadata": {
        "id": "-SYOrvk80rlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_vehicles_between(year_start: int, year_end: int) -> pd.DataFrame:\n",
        "    def _postprocess(d: pd.DataFrame, dt_variant: bool) -> pd.DataFrame:\n",
        "        if d.empty:\n",
        "            return d\n",
        "        if dt_variant:\n",
        "            d[\"first_registration_nl_date\"] = pd.to_datetime(\n",
        "                d[\"datum_eerste_tenaamstelling_in_nederland_dt\"], errors=\"coerce\"\n",
        "            )\n",
        "            d[\"current_registration_date\"] = pd.to_datetime(\n",
        "                d[\"datum_tenaamstelling_dt\"], errors=\"coerce\"\n",
        "            )\n",
        "            d[\"first_adm_date\"] = pd.to_datetime(\n",
        "                d.get(\"datum_eerste_toelating_dt\"), errors=\"coerce\"\n",
        "            )\n",
        "        else:\n",
        "            d[\"first_registration_nl_date\"] = pd.to_datetime(\n",
        "                d[\"datum_eerste_tenaamstelling_in_nederland\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
        "            )\n",
        "            d[\"current_registration_date\"] = pd.to_datetime(\n",
        "                d[\"datum_tenaamstelling\"], format=\"%Y%m%d\", errors=\"coerce\"\n",
        "            )\n",
        "            d[\"first_adm_date\"] = pd.to_datetime(\n",
        "                d.get(\"datum_eerste_toelating\"), format=\"%Y%m%d\", errors=\"coerce\"\n",
        "            )\n",
        "\n",
        "        keep = [\n",
        "            \"kenteken\",\"merk\",\"handelsbenaming\",\"catalogusprijs\",\n",
        "            \"first_registration_nl_date\",\"current_registration_date\",\"zuinigheidsclassificatie\",\n",
        "            \"inrichting\",\n",
        "            \"aantal_zitplaatsen\",\"massa_ledig_voertuig\",\n",
        "            \"lengte\",\"breedte\",\"wielbasis\",\n",
        "            \"vermogen_massarijklaar\",\"massa_rijklaar\",\n",
        "            \"first_adm_date\",\n",
        "        ]\n",
        "        for col in keep:\n",
        "            if col not in d.columns:\n",
        "                d[col] = pd.NA\n",
        "        out = d[keep].copy()\n",
        "\n",
        "        num_cols = [\n",
        "            \"catalogusprijs\",\"aantal_zitplaatsen\",\"massa_ledig_voertuig\",\n",
        "            \"lengte\",\"breedte\",\"wielbasis\",\n",
        "            \"vermogen_massarijklaar\",\"massa_rijklaar\",\n",
        "        ]\n",
        "        for c in num_cols:\n",
        "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
        "\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            ptw = out[\"vermogen_massarijklaar\"] / out[\"massa_rijklaar\"]\n",
        "        out[\"power_to_weight\"] = ptw.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        out[\"datum_eerste_toelating_year\"] = (\n",
        "            pd.to_datetime(out[\"first_adm_date\"], errors=\"coerce\").dt.year.astype(\"Int64\")\n",
        "        )\n",
        "\n",
        "        out[\"kenteken\"] = out[\"kenteken\"].map(_norm_plate)\n",
        "        return out\n",
        "\n",
        "    def _try_dt(include_class: bool) -> pd.DataFrame:\n",
        "        base_fields = [\n",
        "            \"kenteken\",\"merk\",\"handelsbenaming\",\n",
        "            \"(catalogusprijs::number) as catalogusprijs\",\n",
        "            \"datum_eerste_tenaamstelling_in_nederland_dt\",\n",
        "            \"datum_tenaamstelling_dt\",\n",
        "        ]\n",
        "        if include_class:\n",
        "            base_fields.append(\"zuinigheidsclassificatie\")\n",
        "        extra_fields = [\n",
        "            \"inrichting\",\n",
        "            \"(aantal_zitplaatsen::number) as aantal_zitplaatsen\",\n",
        "            \"(massa_ledig_voertuig::number) as massa_ledig_voertuig\",\n",
        "            \"(lengte::number) as lengte\",\n",
        "            \"(breedte::number) as breedte\",\n",
        "            \"(wielbasis::number) as wielbasis\",\n",
        "            \"(vermogen_massarijklaar::number) as vermogen_massarijklaar\",\n",
        "            \"(massa_rijklaar::number) as massa_rijklaar\",\n",
        "            \"datum_eerste_toelating_dt\",\n",
        "        ]\n",
        "        select_dt = \",\".join(base_fields + extra_fields)\n",
        "        where_dt = (\n",
        "            f\"datum_eerste_tenaamstelling_in_nederland_dt >= '{year_start}-01-01T00:00:00' AND \"\n",
        "            f\"datum_eerste_tenaamstelling_in_nederland_dt <  '{year_end+1}-01-01T00:00:00' AND \"\n",
        "            \"catalogusprijs IS NOT NULL AND merk IS NOT NULL AND handelsbenaming IS NOT NULL\"\n",
        "        )\n",
        "        d = fetch_soda(RESOURCE_VEH, select_dt, where_dt, limit=PAGE_LIMIT, max_rows=MAX_ROWS)\n",
        "        return _postprocess(d, True)\n",
        "\n",
        "    def _try_str(include_class: bool) -> pd.DataFrame:\n",
        "        base_fields = [\n",
        "            \"kenteken\",\"merk\",\"handelsbenaming\",\n",
        "            \"(catalogusprijs::number) as catalogusprijs\",\n",
        "            \"datum_eerste_tenaamstelling_in_nederland\",\n",
        "            \"datum_tenaamstelling\",\n",
        "        ]\n",
        "        if include_class:\n",
        "            base_fields.append(\"zuinigheidsclassificatie\")\n",
        "        extra_fields = [\n",
        "            \"inrichting\",\n",
        "            \"(aantal_zitplaatsen::number) as aantal_zitplaatsen\",\n",
        "            \"(massa_ledig_voertuig::number) as massa_ledig_voertuig\",\n",
        "            \"(lengte::number) as lengte\",\n",
        "            \"(breedte::number) as breedte\",\n",
        "            \"(wielbasis::number) as wielbasis\",\n",
        "            \"(vermogen_massarijklaar::number) as vermogen_massarijklaar\",\n",
        "            \"(massa_rijklaar::number) as massa_rijklaar\",\n",
        "            \"datum_eerste_toelating\",\n",
        "        ]\n",
        "        select_str = \",\".join(base_fields + extra_fields)\n",
        "        where_str = (\n",
        "            f\"datum_eerste_tenaamstelling_in_nederland >= '{year_start}0101' AND \"\n",
        "            f\"datum_eerste_tenaamstelling_in_nederland <= '{year_end}1231' AND \"\n",
        "            \"catalogusprijs IS NOT NULL AND merk IS NOT NULL AND handelsbenaming IS NOT NULL\"\n",
        "        )\n",
        "        d = fetch_soda(RESOURCE_VEH, select_str, where_str, limit=PAGE_LIMIT, max_rows=MAX_ROWS)\n",
        "        return _postprocess(d, False)\n",
        "\n",
        "    try:\n",
        "        d = _try_dt(include_class=True)\n",
        "    except requests.HTTPError:\n",
        "        d = pd.DataFrame()\n",
        "    if d.empty:\n",
        "        try:\n",
        "            d = _try_dt(include_class=False)\n",
        "        except requests.HTTPError:\n",
        "            d = pd.DataFrame()\n",
        "    if d.empty:\n",
        "        try:\n",
        "            d = _try_str(include_class=True)\n",
        "        except requests.HTTPError:\n",
        "            d = pd.DataFrame()\n",
        "    if d.empty:\n",
        "        d = _try_str(include_class=False)\n",
        "\n",
        "    if d.empty:\n",
        "        return d\n",
        "\n",
        "    mask = (\n",
        "        d[\"first_registration_nl_date\"].notna() &\n",
        "        (d[\"first_registration_nl_date\"].dt.year >= year_start) &\n",
        "        (d[\"first_registration_nl_date\"].dt.year <= year_end)\n",
        "    )\n",
        "    df = d[mask].copy()\n",
        "\n",
        "    if not df.empty:\n",
        "        fr_min = df[\"first_registration_nl_date\"].min()\n",
        "        fr_max = df[\"first_registration_nl_date\"].max()\n",
        "        cr_min = df[\"current_registration_date\"].min()\n",
        "        cr_max = df[\"current_registration_date\"].max()\n",
        "        print(\"[DEBUG] NL first-registration range:\", fr_min.date(), \"→\", fr_max.date())\n",
        "        print(\"[DEBUG] Current registration range: \", cr_min.date(), \"→\", cr_max.date())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "WLUI523P0_CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _chunk_list(lst: List[str], n: int):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]"
      ],
      "metadata": {
        "id": "R8rtgCK91T98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_fuel_primary_for_plates(kentekens: List[str], batch_size: int = 400) -> pd.DataFrame:\n",
        "    if not kentekens:\n",
        "        return pd.DataFrame(columns=[\"kenteken\", \"brandstof_omschrijving\"])\n",
        "    out = []\n",
        "    for chunk in _chunk_list(list(kentekens), batch_size):\n",
        "        in_list = \",\".join([f\"'{k}'\" for k in chunk])\n",
        "        where = f\"kenteken in ({in_list}) AND brandstof_volgnummer = '1' AND brandstof_omschrijving IS NOT NULL\"\n",
        "        df = fetch_soda(\n",
        "            RESOURCE_FUEL,\n",
        "            select=\"kenteken,brandstof_omschrijving,brandstof_volgnummer\",\n",
        "            where=where,\n",
        "            limit=5000,\n",
        "            max_rows=None,\n",
        "        )\n",
        "        out.append(df)\n",
        "    df_all = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
        "    if df_all.empty:\n",
        "        return pd.DataFrame(columns=[\"kenteken\", \"brandstof_omschrijving\"])\n",
        "    df_all[\"kenteken\"] = df_all[\"kenteken\"].map(_norm_plate)\n",
        "    df_all = (df_all.sort_values([\"kenteken\", \"brandstof_volgnummer\"])\n",
        "                    .drop_duplicates(subset=[\"kenteken\"], keep=\"first\"))\n",
        "    return df_all[[\"kenteken\", \"brandstof_omschrijving\"]]\n"
      ],
      "metadata": {
        "id": "B3dT5BJ61WTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering & aggregation"
      ],
      "metadata": {
        "id": "LFwRoU4c1az5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_occasion_flag(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    a = pd.to_datetime(df.get(\"first_registration_nl_date\"), errors=\"coerce\")\n",
        "    b = pd.to_datetime(df.get(\"current_registration_date\"), errors=\"coerce\")\n",
        "    df[\"occasion_flag\"] = np.where(a.notna() & b.notna() & (a != b), 1, 0).astype(\"int64\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "ND2qpY_l1Zi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_non_null(values):\n",
        "    vals = [str(v).strip() for v in values if pd.notnull(v) and str(v).strip()]\n",
        "    return \", \".join(sorted(set(vals))) if vals else None\n",
        "\n",
        "def mode_non_null(values):\n",
        "    s = pd.Series([str(v).strip() for v in values if pd.notnull(v) and str(v).strip()])\n",
        "    return s.mode().iloc[0] if not s.empty else None\n",
        "\n",
        "def median_non_null(values):\n",
        "    s = pd.to_numeric(pd.Series(values), errors=\"coerce\")\n",
        "    if s.notna().any():\n",
        "        return float(s.median())\n",
        "    return np.nan\n"
      ],
      "metadata": {
        "id": "gk4NZjsh1_WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_brand_model_per_year(df_join: pd.DataFrame,\n",
        "                               year_start: int, year_end: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Brand–model aggregation (per-year) + extras (incl. computed power_to_weight).\n",
        "    \"\"\"\n",
        "    if df_join.empty:\n",
        "        cols = [\"brand\",\"model\",\"fuel_types_primary\",\"economy_rate\",\"resold_flag\",\n",
        "                \"inrichting_std\",\n",
        "                \"seats_median\",\"mass_empty_median\",\"length_median\",\"width_median\",\"wheelbase_median\",\n",
        "                \"pw_ratio_median\",\n",
        "                \"datum_eerste_toelating_year\"] + \\\n",
        "               [f\"count_{y}\" for y in range(year_start, year_end+1)] + \\\n",
        "               [f\"avg_{y}\" for y in range(year_start, year_end+1)]\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    df = df_join.copy()\n",
        "\n",
        "    df[\"catalogusprijs\"] = pd.to_numeric(df[\"catalogusprijs\"], errors=\"coerce\")\n",
        "    df = df[df[\"catalogusprijs\"].notna() & (df[\"catalogusprijs\"] > 0)]\n",
        "    df[\"merk\"] = df[\"merk\"].astype(str).str.strip()\n",
        "    df[\"handelsbenaming\"] = df[\"handelsbenaming\"].astype(str).str.strip()\n",
        "\n",
        "    if \"occasion_flag\" not in df.columns:\n",
        "        df = compute_occasion_flag(df)\n",
        "\n",
        "    if \"power_to_weight\" not in df.columns:\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            df[\"power_to_weight\"] = pd.to_numeric(df.get(\"vermogen_massarijklaar\"), errors=\"coerce\") / \\\n",
        "                                    pd.to_numeric(df.get(\"massa_rijklaar\"), errors=\"coerce\")\n",
        "        df[\"power_to_weight\"] = df[\"power_to_weight\"].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    df[\"year\"] = pd.to_datetime(df[\"first_registration_nl_date\"], errors=\"coerce\").dt.year\n",
        "    df = df[(df[\"year\"].notna()) & (df[\"year\"] >= year_start) & (df[\"year\"] <= year_end)]\n",
        "\n",
        "    counts = (df.groupby([\"merk\",\"handelsbenaming\",\"year\"], as_index=False)\n",
        "                .agg(vehicle_count=(\"kenteken\", \"nunique\")))\n",
        "    pivot_counts = (counts.pivot_table(index=[\"merk\",\"handelsbenaming\"], columns=\"year\",\n",
        "                                       values=\"vehicle_count\", fill_value=0)\n",
        "                          .rename(columns={y: f\"count_{y}\" for y in counts[\"year\"].unique()})\n",
        "                          .reset_index())\n",
        "\n",
        "    avgs = (df.groupby([\"merk\",\"handelsbenaming\",\"year\"], as_index=False)\n",
        "              .agg(avg_price=(\"catalogusprijs\",\"mean\")))\n",
        "    pivot_avgs = (avgs.pivot_table(index=[\"merk\",\"handelsbenaming\"], columns=\"year\",\n",
        "                                   values=\"avg_price\")\n",
        "                        .rename(columns={y: f\"avg_{y}\" for y in avgs[\"year\"].unique()})\n",
        "                        .reset_index())\n",
        "\n",
        "    per_year = pivot_counts.merge(pivot_avgs, on=[\"merk\",\"handelsbenaming\"], how=\"left\")\n",
        "\n",
        "    for y in range(year_start, year_end+1):\n",
        "        ccol, acol = f\"count_{y}\", f\"avg_{y}\"\n",
        "        if ccol not in per_year.columns:\n",
        "            per_year[ccol] = 0\n",
        "        if acol not in per_year.columns:\n",
        "            per_year[acol] = np.nan\n",
        "        per_year.loc[per_year[ccol] == 0, acol] = np.nan\n",
        "        per_year[acol] = pd.to_numeric(per_year[acol], errors=\"coerce\").round(2)\n",
        "\n",
        "    fuel_agg = (df.groupby([\"merk\",\"handelsbenaming\"], as_index=False)\n",
        "                  .agg(fuel_types_primary=(\"brandstof_omschrijving\", unique_non_null)))\n",
        "\n",
        "    econ_agg = (df.groupby([\"merk\",\"handelsbenaming\"], as_index=False)\n",
        "                  .agg(economy_rate=(\"zuinigheidsclassificatie\", mode_non_null)))\n",
        "\n",
        "    resold = (df.groupby([\"merk\",\"handelsbenaming\"], as_index=False)\n",
        "                .agg(resold_flag=(\"occasion_flag\", lambda x: int((pd.Series(x) == 1).any()))))\n",
        "\n",
        "    text_agg = (df.groupby([\"merk\",\"handelsbenaming\"], as_index=False)\n",
        "                  .agg(inrichting_std=(\"inrichting\", mode_non_null)))\n",
        "\n",
        "    med_agg = (df.groupby([\"merk\",\"handelsbenaming\"], as_index=False)\n",
        "                 .agg(seats_median=(\"aantal_zitplaatsen\", median_non_null),\n",
        "                      mass_empty_median=(\"massa_ledig_voertuig\", median_non_null),\n",
        "                      length_median=(\"lengte\", median_non_null),\n",
        "                      width_median=(\"breedte\", median_non_null),\n",
        "                      wheelbase_median=(\"wielbasis\", median_non_null),\n",
        "                      pw_ratio_median=(\"power_to_weight\", median_non_null)))\n",
        "\n",
        "    year_ref = (df.groupby([\"merk\",\"handelsbenaming\"], as_index=False)\n",
        "                  .agg(datum_eerste_toelating_year=(\"datum_eerste_toelating_year\",\n",
        "                                                    lambda v: pd.Series(v).mode().iloc[0]\n",
        "                                                    if len(pd.Series(v).dropna()) else np.nan)))\n",
        "\n",
        "    grouped = (per_year\n",
        "               .merge(fuel_agg, on=[\"merk\",\"handelsbenaming\"], how=\"left\")\n",
        "               .merge(econ_agg, on=[\"merk\",\"handelsbenaming\"], how=\"left\")\n",
        "               .merge(resold,  on=[\"merk\",\"handelsbenaming\"], how=\"left\")\n",
        "               .merge(text_agg, on=[\"merk\",\"handelsbenaming\"], how=\"left\")\n",
        "               .merge(med_agg,  on=[\"merk\",\"handelsbenaming\"], how=\"left\")\n",
        "               .merge(year_ref, on=[\"merk\",\"handelsbenaming\"], how=\"left\"))\n",
        "\n",
        "    if \"datum_eerste_toelating_year\" in grouped.columns:\n",
        "        grouped[\"datum_eerste_toelating_year\"] = (\n",
        "            pd.to_numeric(grouped[\"datum_eerste_toelating_year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "        )\n",
        "\n",
        "    for y in range(year_start, year_end+1):\n",
        "        for prefix in (\"count\", \"avg\"):\n",
        "            col = f\"{prefix}_{y}\"\n",
        "            if col not in grouped.columns:\n",
        "                grouped[col] = 0 if prefix == \"count\" else np.nan\n",
        "\n",
        "    grouped = grouped.rename(columns={\"merk\":\"brand\",\"handelsbenaming\":\"model\"})\n",
        "    cols = [\"brand\",\"model\",\n",
        "            \"fuel_types_primary\",\"economy_rate\",\"resold_flag\",\n",
        "            \"inrichting_std\",\n",
        "            \"seats_median\",\"mass_empty_median\",\"length_median\",\"width_median\",\"wheelbase_median\",\n",
        "            \"pw_ratio_median\",\n",
        "            \"datum_eerste_toelating_year\"] + \\\n",
        "           [f\"count_{y}\" for y in range(year_start, year_end+1)] + \\\n",
        "           [f\"avg_{y}\"   for y in range(year_start, year_end+1)]\n",
        "    grouped = grouped[cols].sort_values(\n",
        "        [f\"count_{year_end}\", f\"count_{max(year_start, year_end-1)}\", \"brand\", \"model\"],\n",
        "        ascending=[False, False, True, True]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    return grouped"
      ],
      "metadata": {
        "id": "Gf-P_Hvg2BRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\")  # keep \\t \\n \\r, drop others\n",
        "\n",
        "def _scrub_text(x: Any) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Return a safe UTF-8 JSON-friendly string:\n",
        "    - convert to str (if not None)\n",
        "    - normalize (NFC)\n",
        "    - remove control chars except TAB/CR/LF\n",
        "    - collapse odd whitespace\n",
        "    \"\"\"\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return None\n",
        "    s = str(x)\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = _CTRL_RE.sub(\"\", s)\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
        "    return s if s else None"
      ],
      "metadata": {
        "id": "GVUC05YDBJ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sanitize_for_bigquery(df: pd.DataFrame, year_start: int, year_end: int) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    int_cols = [f\"count_{y}\" for y in range(year_start, year_end + 1)]\n",
        "    if \"resold_flag\" in out.columns:\n",
        "        int_cols.append(\"resold_flag\")\n",
        "    if \"datum_eerste_toelating_year\" in out.columns:\n",
        "        int_cols.append(\"datum_eerste_toelating_year\")\n",
        "    for c in int_cols:\n",
        "        if c in out.columns:\n",
        "            s = pd.to_numeric(out[c], errors=\"coerce\").astype(\"Int64\")\n",
        "            out[c] = s.apply(lambda v: int(v) if pd.notna(v) else None)\n",
        "\n",
        "    float_cols = [\n",
        "        \"seats_median\",\"mass_empty_median\",\"length_median\",\"width_median\",\"wheelbase_median\",\n",
        "        \"pw_ratio_median\",\n",
        "        *[f\"avg_{y}\" for y in range(year_start, year_end + 1)],\n",
        "    ]\n",
        "    for c in float_cols:\n",
        "        if c in out.columns:\n",
        "            s = pd.to_numeric(out[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
        "            out[c] = s.apply(lambda v: float(v) if pd.notna(v) else None)\n",
        "\n",
        "    exp_str_cols = [\"brand\",\"model\",\"fuel_types_primary\",\"economy_rate\",\"inrichting_std\",\"image_url\"]\n",
        "    for c in exp_str_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].apply(_scrub_text)\n",
        "\n",
        "    for c in out.columns:\n",
        "        if c not in int_cols and c not in float_cols:\n",
        "            if out[c].dtype == \"object\":\n",
        "                out[c] = out[c].apply(_scrub_text)\n",
        "\n",
        "    out = out.replace([np.inf, -np.inf], np.nan)\n",
        "    out = out.where(pd.notnull(out), None)\n",
        "    return out"
      ],
      "metadata": {
        "id": "TN95Hz2B4KwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google custom search (url images)"
      ],
      "metadata": {
        "id": "SLUx7xQI2JLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures as _fut\n",
        "\n",
        "# Domains: higher is better\n",
        "_DOMAIN_WEIGHTS = {\n",
        "    # Manufacturer / press\n",
        "    \"media.mercedes-benz.com\": 10,\n",
        "    \"press.bmwgroup.com\": 10,\n",
        "    \"bmw.com\": 9,\n",
        "    \"mercedes-benz.com\": 9,\n",
        "    \"volkswagen-newsroom.com\": 9,\n",
        "    \"toyota.eu\": 8,\n",
        "    \"press.toyota\": 8,\n",
        "    # Reputable automotive media\n",
        "    \"topgear.com\": 7,\n",
        "    \"autocar.co.uk\": 7,\n",
        "    \"caranddriver.com\": 7,\n",
        "    \"motor1.com\": 7,\n",
        "    \"ev-database.nl\": 7,\n",
        "    \"autoweek.nl\": 7,\n",
        "    \"autoweek.com\": 6,\n",
        "    \"autoblog.nl\": 6,\n",
        "    \"whatcar.com\": 6,\n",
        "    # Reference\n",
        "    \"wikipedia.org\": 8,\n",
        "    \"wikimedia.org\": 8,\n",
        "    # Marketplaces/stock (downweight, still acceptable)\n",
        "    \"autotrader\": 3,\n",
        "    \"cars.com\": 3,\n",
        "    \"mobile.de\": 3,\n",
        "    \"marktplaats.nl\": 2,\n",
        "}\n",
        "\n",
        "_BAD_TOKENS = [\n",
        "    \"render\", \"concept\", \"spyshot\", \"spy-shot\", \"camouflage\", \"tuning\", \"widebody\",\n",
        "    \"lego\", \"forza\", \"gran-turismo\", \"gta\", \"assetto\", \"mod\", \"wrap\", \"damage\",\n",
        "]\n",
        "\n",
        "def _domain_weight(url: str) -> int:\n",
        "    try:\n",
        "        from urllib.parse import urlparse\n",
        "        host = urlparse(url).netloc.lower()\n",
        "    except Exception:\n",
        "        return 0\n",
        "    score = 0\n",
        "    for dom, w in _DOMAIN_WEIGHTS.items():\n",
        "        if dom in host:\n",
        "            score = max(score, w)\n",
        "    return score\n",
        "\n",
        "def _tokenize(s: str) -> List[str]:\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = re.sub(r\"[_\\-.,/]\", \" \", s)\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    return [t for t in s.lower().split() if t]\n",
        "\n",
        "def _contains_bad_tokens(s: str) -> bool:\n",
        "    low = s.lower()\n",
        "    return any(bt in low for bt in _BAD_TOKENS)\n",
        "\n",
        "def _google_search_images(brand: str, model: str, year_hint: Optional[int],\n",
        "                          body_kw: List[str], fuel_kw: List[str],\n",
        "                          api_key: str, cx: str,\n",
        "                          timeout: int = 12, num: int = 10) -> List[Dict[str, Any]]:\n",
        "    # Build base query variants\n",
        "    base = f\"{brand} {model}\".strip()\n",
        "    q_variants = []\n",
        "    if year_hint:\n",
        "        q_variants += [\n",
        "            f\"{base} {year_hint}\",\n",
        "            f\"{base} {year_hint} photo\",\n",
        "        ]\n",
        "    q_variants += [base, f\"{base} car\"]\n",
        "\n",
        "    # Include body/fuel hints\n",
        "    hints = []\n",
        "    if body_kw:\n",
        "        hints.append(\" \".join(body_kw))\n",
        "    if fuel_kw:\n",
        "        hints.append(\" \".join(fuel_kw))\n",
        "    if hints:\n",
        "        q_variants = [f\"{q} {' '.join(hints)}\" for q in q_variants]\n",
        "\n",
        "    seen = set()\n",
        "    items: List[Dict[str, Any]] = []\n",
        "    for q in q_variants:\n",
        "        try:\n",
        "            r = requests.get(\n",
        "                \"https://www.googleapis.com/customsearch/v1\",\n",
        "                params={\n",
        "                    \"q\": q,\n",
        "                    \"searchType\": \"image\",\n",
        "                    \"num\": max(1, min(num, 10)),\n",
        "                    \"safe\": \"active\",\n",
        "                    \"key\": api_key,\n",
        "                    \"cx\": cx,\n",
        "                    \"gl\": \"nl\",                  # country bias\n",
        "                    \"lr\": \"lang_nl|lang_en\",     # language bias\n",
        "                    \"imgType\": \"photo\",          # prefer real photos\n",
        "                },\n",
        "                timeout=timeout,\n",
        "            )\n",
        "            if not r.ok:\n",
        "                continue\n",
        "            for it in (r.json() or {}).get(\"items\", []) or []:\n",
        "                link = it.get(\"link\") or (it.get(\"image\") or {}).get(\"thumbnailLink\")\n",
        "                if not link or link in seen:\n",
        "                    continue\n",
        "                title = it.get(\"title\") or \"\"\n",
        "                context = it.get(\"image\", {}).get(\"contextLink\") or \"\"\n",
        "                items.append({\"link\": link, \"title\": title, \"context\": context, \"q\": q})\n",
        "                seen.add(link)\n",
        "        except requests.RequestException:\n",
        "            pass\n",
        "        # Small delay between variants to be nice to API; increase if you see 429s\n",
        "        time.sleep(0.05)\n",
        "    return items\n",
        "\n",
        "def _score_candidate(link: str, title: str, context: str,\n",
        "                     brand: str, model_canon: str,\n",
        "                     body_kw: List[str], year_hint: Optional[int]) -> float:\n",
        "    # base tokens\n",
        "    want_tokens = set(_tokenize(f\"{brand} {model_canon}\"))\n",
        "    text = \" \".join([link, title, context])\n",
        "    have_tokens = set(_tokenize(text))\n",
        "\n",
        "    # token overlap\n",
        "    overlap = len(want_tokens & have_tokens)\n",
        "    score = overlap * 3.0\n",
        "\n",
        "    # domain quality\n",
        "    score += _domain_weight(link) * 1.5\n",
        "\n",
        "    # body style bonus\n",
        "    if body_kw:\n",
        "        if any(b in have_tokens for b in body_kw):\n",
        "            score += 2.0\n",
        "\n",
        "    # year bonus (not hard filter)\n",
        "    if year_hint:\n",
        "        if str(year_hint) in text:\n",
        "            score += 1.5\n",
        "\n",
        "    # penalties\n",
        "    if _contains_bad_tokens(text):\n",
        "        score -= 5.0\n",
        "\n",
        "    # very small bonus if title contains exact model string\n",
        "    if model_canon.lower() in title.lower():\n",
        "        score += 0.8\n",
        "\n",
        "    return score\n",
        "\n",
        "def _pick_best_image_smart(brand_raw: str, model_raw: str,\n",
        "                           year_hint: Optional[int],\n",
        "                           inrichting: Optional[str],\n",
        "                           fuel_text: Optional[str],\n",
        "                           api_key: str, cx: str,\n",
        "                           validate_urls: bool = False,\n",
        "                           fallback_openverse: bool = True) -> Optional[str]:\n",
        "    brand = _canon_brand(brand_raw)\n",
        "    model_canon = _canon_model_tokens(model_raw)\n",
        "\n",
        "    body_kw = _inrichting_to_body_keywords(inrichting)\n",
        "    fuel_kw = []\n",
        "    if fuel_text:\n",
        "        ft = fuel_text.lower()\n",
        "        if \"elektr\" in ft or \"electric\" in ft or \"bev\" in ft:\n",
        "            fuel_kw.append(\"electric\")\n",
        "        elif \"hybr\" in ft or \"plug\" in ft or \"phev\" in ft:\n",
        "            fuel_kw.append(\"hybrid\")\n",
        "        elif \"diesel\" in ft or \"d\" == ft.strip():\n",
        "            fuel_kw.append(\"diesel\")\n",
        "        elif \"benzine\" in ft or \"petrol\" in ft or \"gasoline\" in ft:\n",
        "            fuel_kw.append(\"petrol\")\n",
        "\n",
        "    items = _google_search_images(brand, model_canon, year_hint, body_kw, fuel_kw,\n",
        "                                  api_key, cx, timeout=12, num=10)\n",
        "    if not items and fallback_openverse:\n",
        "        return _openverse_first(brand, model_canon)\n",
        "\n",
        "    # score and choose best\n",
        "    best_url = None\n",
        "    best_score = -1e9\n",
        "    for it in items:\n",
        "        url = it[\"link\"]\n",
        "        title = it[\"title\"] or \"\"\n",
        "        ctx = it[\"context\"] or \"\"\n",
        "        sc = _score_candidate(url, title, ctx, brand, model_canon, body_kw, year_hint)\n",
        "        if validate_urls and sc > best_score:\n",
        "            if not _head_ok(url, timeout=8):\n",
        "                continue\n",
        "        if sc > best_score:\n",
        "            best_score = sc\n",
        "            best_url = url\n",
        "\n",
        "    if best_url is None and fallback_openverse:\n",
        "        return _openverse_first(brand, model_canon)\n",
        "    return best_url"
      ],
      "metadata": {
        "id": "jNwgymJo2Fj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _head_ok(url: str, timeout: int = 6) -> bool:\n",
        "    try:\n",
        "        h = requests.head(url, allow_redirects=True, timeout=timeout)\n",
        "        if 200 <= h.status_code < 400:\n",
        "            return True\n",
        "        g = requests.get(url, stream=True, allow_redirects=True, timeout=timeout)\n",
        "        return 200 <= g.status_code < 400\n",
        "    except requests.RequestException:\n",
        "        return False\n",
        "\n",
        "def _openverse_first(brand: str, model: str) -> Optional[str]:\n",
        "    try:\n",
        "        resp = requests.get(\n",
        "            \"https://api.openverse.engineering/v1/images\",\n",
        "            params={\"q\": f\"{brand} {model} car\", \"page_size\": 1},\n",
        "            timeout=8,\n",
        "        )\n",
        "        if resp.ok:\n",
        "            res = (resp.json() or {}).get(\"results\", [])\n",
        "            if res:\n",
        "                return res[0].get(\"thumbnail\") or res[0].get(\"url\")\n",
        "    except requests.RequestException:\n",
        "        pass\n",
        "    return None"
      ],
      "metadata": {
        "id": "3ay2CbSz2Pxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_image_column_google_parallel_smart(\n",
        "    df_agg: pd.DataFrame,\n",
        "    api_key: str,\n",
        "    cx: str,\n",
        "    *,\n",
        "    max_workers: int = 10,           # increase if you have quota\n",
        "    per_request_delay: float = 0.0,  # leave 0 when parallel\n",
        "    validate_urls: bool = False,     # HEAD check; slower\n",
        "    fallback_openverse: bool = True,\n",
        "    top_n: Optional[int] = None,\n",
        "    use_empty_list_if_missing: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    work = df_agg.copy()\n",
        "    if top_n:\n",
        "        sort_keys = [k for k in [f\"count_{YEAR_END}\", \"brand\", \"model\"] if k in work.columns]\n",
        "        work = work.sort_values(sort_keys, ascending=[False, True, True]).head(top_n).copy()\n",
        "\n",
        "    missing_sentinel = \"[]\" if use_empty_list_if_missing else None\n",
        "\n",
        "    if not api_key or not cx:\n",
        "        print(\"[WARN] GOOGLE_API_KEY or GOOGLE_CSE_ID missing. Skipping image enrichment.\")\n",
        "        work[\"image_url\"] = missing_sentinel\n",
        "        if top_n:\n",
        "            full = df_agg.merge(work[[\"brand\",\"model\",\"image_url\"]], on=[\"brand\",\"model\"], how=\"left\")\n",
        "            if use_empty_list_if_missing:\n",
        "                full[\"image_url\"] = full[\"image_url\"].fillna(missing_sentinel)\n",
        "            return full\n",
        "        return work\n",
        "\n",
        "    pairs = []\n",
        "    for _, r in work.iterrows():\n",
        "        pairs.append((\n",
        "            str(r[\"brand\"]), str(r[\"model\"]),\n",
        "            int(r[\"datum_eerste_toelating_year\"]) if pd.notna(r.get(\"datum_eerste_toelating_year\")) else None,\n",
        "            r.get(\"inrichting_std\"),\n",
        "            r.get(\"fuel_types_primary\"),\n",
        "        ))\n",
        "\n",
        "    # Deduplicate (brand, model, year, body, fuel) to reduce API calls\n",
        "    uniq_pairs = list(dict.fromkeys(pairs))\n",
        "\n",
        "    cache: Dict[Tuple[str, str, Optional[int], Optional[str], Optional[str]], Optional[str]] = {}\n",
        "\n",
        "    def _task(bm_y_body_fuel: Tuple[str, str, Optional[int], Optional[str], Optional[str]]\n",
        "             ) -> Tuple[Tuple[str, str, Optional[int], Optional[str], Optional[str]], Optional[str]]:\n",
        "        brand, model, year_hint, inr, fuel = bm_y_body_fuel\n",
        "        if per_request_delay > 0:\n",
        "            time.sleep(per_request_delay)\n",
        "        url = _pick_best_image_smart(\n",
        "            brand, model, year_hint, inr, fuel,\n",
        "            api_key=api_key, cx=cx,\n",
        "            validate_urls=validate_urls,\n",
        "            fallback_openverse=fallback_openverse,\n",
        "        )\n",
        "        return bm_y_body_fuel, url\n",
        "\n",
        "    if uniq_pairs:\n",
        "        with _fut.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "            for key, url in ex.map(_task, uniq_pairs, chunksize=max(1, len(uniq_pairs)//(max_workers*4))):\n",
        "                cache[key] = url\n",
        "\n",
        "    urls = []\n",
        "    for key in pairs:\n",
        "        chosen = cache.get(key)\n",
        "        if chosen is None and use_empty_list_if_missing:\n",
        "            chosen = missing_sentinel\n",
        "        urls.append(chosen)\n",
        "\n",
        "    work = work.assign(image_url=urls)\n",
        "\n",
        "    if top_n:\n",
        "        full = df_agg.merge(work[[\"brand\",\"model\",\"image_url\"]], on=[\"brand\",\"model\"], how=\"left\")\n",
        "        if use_empty_list_if_missing:\n",
        "            full[\"image_url\"] = full[\"image_url\"].fillna(missing_sentinel)\n",
        "        return full\n",
        "\n",
        "    return work"
      ],
      "metadata": {
        "id": "X43dMu7B2XDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _canon_model_tokens(model: str) -> str:\n",
        "    s = str(model).upper().strip()\n",
        "    # common RDW-to-consumer fixes\n",
        "    replacements = {\n",
        "        r\"\\bA KLASSE\\b\": \"A-Class\",\n",
        "        r\"\\bB KLASSE\\b\": \"B-Class\",\n",
        "        r\"\\bC KLASSE\\b\": \"C-Class\",\n",
        "        r\"\\bE KLASSE\\b\": \"E-Class\",\n",
        "        r\"\\bS KLASSE\\b\": \"S-Class\",\n",
        "        r\"\\b3 SERIE\\b\": \"3 Series\",\n",
        "        r\"\\b5 SERIE\\b\": \"5 Series\",\n",
        "        r\"\\b7 SERIE\\b\": \"7 Series\",\n",
        "        r\"\\bID3\\b\": \"ID.3\",\n",
        "        r\"\\bID4\\b\": \"ID.4\",\n",
        "        r\"\\bID5\\b\": \"ID.5\",\n",
        "        r\"\\bC HR\\b\": \"C-HR\",\n",
        "        r\"\\bCX 5\\b\": \"CX-5\",\n",
        "        r\"\\bCX 3\\b\": \"CX-3\",\n",
        "        r\"\\bQASHQAI\\b\": \"Qashqai\",\n",
        "        r\"\\bKUGA PLUG IN\\b\": \"Kuga Plug-in\",\n",
        "        r\"\\bPLUG[- ]?IN\\b\": \"Plug-in\",\n",
        "        r\"\\bHYBRID\\b\": \"Hybrid\",\n",
        "    }\n",
        "    for pat, rep in replacements.items():\n",
        "        s = re.sub(pat, rep, s)\n",
        "    # kill common engine codes / junk tokens\n",
        "    s = re.sub(r\"\\b(\\d\\.\\d|TSI|TFSI|TDI|CDI|dCi|DCI|HDI|BlueHDi|BlueTec|MHEV|PHEV|HEV|BEV|AWD|4MATIC)\\b\", \"\", s, flags=re.I)\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _canon_brand(brand: str) -> str:\n",
        "    # Keep brand case nicely\n",
        "    known = {\n",
        "        \"VW\": \"Volkswagen\",\n",
        "        \"MB\": \"Mercedes-Benz\",\n",
        "        \"MERCEDES\": \"Mercedes-Benz\",\n",
        "        \"BMW\": \"BMW\",\n",
        "        \"TOYOTA\": \"Toyota\",\n",
        "        \"VOLVO\": \"Volvo\",\n",
        "        \"PEUGEOT\": \"Peugeot\",\n",
        "        \"CITROEN\": \"Citroën\",\n",
        "        \"CITROËN\": \"Citroën\",\n",
        "        \"OPEL\": \"Opel\",\n",
        "        \"RENAULT\": \"Renault\",\n",
        "        \"ŠKODA\": \"Škoda\",\n",
        "        \"SKODA\": \"Škoda\",\n",
        "    }\n",
        "    s = str(brand).strip()\n",
        "    up = s.upper()\n",
        "    return known.get(up, s)\n",
        "\n",
        "def _inrichting_to_body_keywords(inrichting: Optional[str]) -> List[str]:\n",
        "    if not inrichting or not str(inrichting).strip():\n",
        "        return []\n",
        "    s = str(inrichting).lower()\n",
        "    # very rough mapping\n",
        "    pairs = [\n",
        "        (\"hatchback\", [\"hatch\", \"hb\"]),\n",
        "        (\"sedan\", [\"sedan\", \"saloon\"]),\n",
        "        (\"wagon\", [\"station\", \"estate\", \"kombi\", \"touring\"]),\n",
        "        (\"suv\", [\"suv\", \"mpv\", \"crossover\"]),\n",
        "        (\"coupe\", [\"coupe\", \"coupé\"]),\n",
        "        (\"convertible\", [\"cabrio\", \"convertible\", \"roadster\"]),\n",
        "        (\"van\", [\"bestel\", \"van\"]),\n",
        "    ]\n",
        "    out = []\n",
        "    for key, toks in pairs:\n",
        "        if any(t in s for t in toks + [key]):\n",
        "            out.append(key)\n",
        "    return out"
      ],
      "metadata": {
        "id": "3McNuBbQTBTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "W0D6volq2b5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    if PROJECT_ID == \"YOUR_GCP_PROJECT_ID\":\n",
        "        print(\"Please set PROJECT_ID env var or edit the script with your GCP project id.\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    client = make_bq_client(PROJECT_ID)  # your existing function\n",
        "    ensure_dataset(client, DATASET_ID, LOCATION)\n",
        "    dataset_ref = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
        "\n",
        "    veh_csv     = f\"vehicles_{YEAR_START}_{YEAR_END}.csv\"\n",
        "    join_csv    = f\"vehicles_with_fuel_{YEAR_START}_{YEAR_END}.csv\"\n",
        "    agg_csv     = f\"brand_model_peryear_{YEAR_START}_{YEAR_END}.csv\"\n",
        "    agg_img_csv = f\"brand_model_peryear_with_images_{YEAR_START}_{YEAR_END}.csv\"\n",
        "\n",
        "    # Vehicles\n",
        "    if os.path.exists(veh_csv):\n",
        "        print(f\"[LOAD] Found {veh_csv}; loading it instead of re-fetching.\")\n",
        "        parse_cols = [\"first_registration_nl_date\", \"current_registration_date\", \"first_adm_date\"]\n",
        "        present_parse = [c for c in parse_cols if c in pd.read_csv(veh_csv, nrows=0).columns]\n",
        "        df_veh = pd.read_csv(veh_csv, parse_dates=present_parse, low_memory=False)\n",
        "        print(f\"[LOAD] vehicles rows: {len(df_veh)}\")\n",
        "    else:\n",
        "        print(f\"[RDW] Fetching vehicles between {YEAR_START} and {YEAR_END} ...\")\n",
        "        df_veh = fetch_vehicles_between(YEAR_START, YEAR_END)\n",
        "        print(f\"[RDW] vehicles rows: {len(df_veh)}\")\n",
        "        if df_veh.empty:\n",
        "            print(\"[ETL] No vehicles found; nothing to write.\")\n",
        "            return\n",
        "        df_veh.to_csv(veh_csv, index=False)\n",
        "        print(f\"[SAVE] Wrote {veh_csv}\")\n",
        "\n",
        "    if \"kenteken\" in df_veh.columns:\n",
        "        df_veh[\"kenteken\"] = df_veh[\"kenteken\"].map(_norm_plate)\n",
        "    df_veh = compute_occasion_flag(df_veh)\n",
        "\n",
        "    # Join with fuel or load\n",
        "    if os.path.exists(join_csv):\n",
        "        print(f\"[LOAD] Found {join_csv}; loading it instead of re-fetching fuel.\")\n",
        "        parse_cols = [\"first_registration_nl_date\", \"current_registration_date\", \"first_adm_date\"]\n",
        "        present_parse = [c for c in parse_cols if c in pd.read_csv(join_csv, nrows=0).columns]\n",
        "        df_join = pd.read_csv(join_csv, parse_dates=present_parse, low_memory=False)\n",
        "        print(f\"[LOAD] vehicles_with_fuel rows: {len(df_join)}\")\n",
        "        if \"kenteken\" in df_join.columns:\n",
        "            df_join[\"kenteken\"] = df_join[\"kenteken\"].map(_norm_plate)\n",
        "    else:\n",
        "        plates = df_veh[\"kenteken\"].dropna().unique().tolist() if \"kenteken\" in df_veh.columns else []\n",
        "        df_fuel = fetch_fuel_primary_for_plates(plates, batch_size=400)\n",
        "        print(f\"[RDW] fuel rows: {len(df_fuel)}\")\n",
        "        right = df_fuel if not df_fuel.empty else pd.DataFrame(columns=[\"kenteken\",\"brandstof_omschrijving\"])\n",
        "        df_join = df_veh.merge(right, on=\"kenteken\", how=\"left\")\n",
        "        df_join.to_csv(join_csv, index=False)\n",
        "        print(f\"[SAVE] Wrote {join_csv}\")\n",
        "        print(f\"[INFO] vehicles_with_fuel rows: {len(df_join)}\")\n",
        "\n",
        "    if \"brandstof_omschrijving\" in df_join.columns:\n",
        "        non_null_fuel = df_join[\"brandstof_omschrijving\"].notna().sum()\n",
        "        print(f\"[CHECK] rows with primary fuel present: {non_null_fuel} / {len(df_join)}\")\n",
        "\n",
        "    # Aggregate\n",
        "    df_agg = build_brand_model_per_year(df_join, YEAR_START, YEAR_END)\n",
        "    print(f\"[AGG] Aggregated rows: {len(df_agg)}\")\n",
        "    if df_agg.empty:\n",
        "        print(\"[ETL] Aggregation empty; nothing to write.\")\n",
        "        return\n",
        "\n",
        "    df_agg.to_csv(agg_csv, index=False)\n",
        "    print(f\"[SAVE] Wrote {agg_csv}\")\n",
        "\n",
        "    # Smart, parallel image enrichment (better quality; slightly slower)\n",
        "    df_agg = add_image_column_google_parallel_smart(\n",
        "        df_agg,\n",
        "        api_key=GOOGLE_API_KEY,\n",
        "        cx=GOOGLE_CSE_ID,\n",
        "        max_workers=10,\n",
        "        per_request_delay=0.0,\n",
        "        validate_urls=IMG_VALIDATE,\n",
        "        fallback_openverse=IMG_FALLBACK_OV,\n",
        "        top_n=IMG_TOP_N,\n",
        "        use_empty_list_if_missing=True,\n",
        "    )\n",
        "\n",
        "    df_agg.to_csv(agg_img_csv, index=False)\n",
        "    print(f\"[SAVE] Wrote {agg_img_csv}\")\n",
        "\n",
        "    # Light numeric cast (optional readability)\n",
        "    for y in range(YEAR_START, YEAR_END + 1):\n",
        "        c = f\"count_{y}\"\n",
        "        a = f\"avg_{y}\"\n",
        "        if c in df_agg.columns:\n",
        "            df_agg[c] = pd.to_numeric(df_agg[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "        if a in df_agg.columns:\n",
        "            df_agg[a] = pd.to_numeric(df_agg[a], errors=\"coerce\").round(2)\n",
        "    if \"resold_flag\" in df_agg.columns:\n",
        "        df_agg[\"resold_flag\"] = pd.to_numeric(df_agg[\"resold_flag\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "\n",
        "    # Sanitize strictly for BigQuery\n",
        "    df_bq = _sanitize_for_bigquery(df_agg, YEAR_START, YEAR_END)\n",
        "    rows = df_bq.to_dict(orient=\"records\")\n",
        "\n",
        "    count_fields = [bigquery.SchemaField(f\"count_{y}\", \"INT64\")  for y in range(YEAR_START, YEAR_END+1)]\n",
        "    avg_fields   = [bigquery.SchemaField(f\"avg_{y}\",   \"FLOAT\")  for y in range(YEAR_START, YEAR_END+1)]\n",
        "\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"brand\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"model\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"fuel_types_primary\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"economy_rate\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"resold_flag\", \"INT64\"),\n",
        "        bigquery.SchemaField(\"inrichting_std\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"seats_median\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"mass_empty_median\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"length_median\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"width_median\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"wheelbase_median\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"pw_ratio_median\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"datum_eerste_toelating_year\", \"INT64\"),\n",
        "        *count_fields,\n",
        "        *avg_fields,\n",
        "        bigquery.SchemaField(\"image_url\", \"STRING\"),\n",
        "    ]\n",
        "\n",
        "    table_id = f\"{dataset_ref}.{TBL_AGG}\"\n",
        "    upload_to_bigquery(\n",
        "        client,\n",
        "        rows,\n",
        "        table_id,\n",
        "        schema=schema,\n",
        "        write_disposition=\"WRITE_TRUNCATE\",\n",
        "    )\n",
        "\n",
        "    print(f\"[DONE] BigQuery table written: {table_id}\")\n",
        "    print(f\"       Window: {YEAR_START}..{YEAR_END}\")\n",
        "    print(f\"       CSVs: {veh_csv}, {join_csv}, {agg_csv}, {agg_img_csv}\")"
      ],
      "metadata": {
        "id": "Xm37PjkO2a8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nInterrupted by user.\", file=sys.stderr)\n",
        "        sys.exit(130)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nUnexpected error: {e}\", file=sys.stderr)\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i7O9uN1f2mKC",
        "outputId": "68e584a2-5a87-4271-b685-3f1a385a3588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BQ] Dataset exists: compact-garage-473209-u4.RDW_final\n",
            "[LOAD] Found vehicles_2023_2025.csv; loading it instead of re-fetching.\n",
            "[LOAD] vehicles rows: 2108141\n",
            "[LOAD] Found vehicles_with_fuel_2023_2025.csv; loading it instead of re-fetching fuel.\n",
            "[LOAD] vehicles_with_fuel rows: 2108141\n",
            "[CHECK] rows with primary fuel present: 2108141 / 2108141\n",
            "[AGG] Aggregated rows: 13326\n",
            "[SAVE] Wrote brand_model_peryear_2023_2025.csv\n",
            "[SAVE] Wrote brand_model_peryear_with_images_2023_2025.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Unexpected error: 400 Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 2; errors: 1. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 2; errors: 1. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: JSON processing encountered too many errors, giving up. Rows: 2; errors: 1; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: JSON parsing error in row starting at position 589: Parser terminated before end of string\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-230879549.py\", line 3, in <cell line: 0>\n",
            "    main()\n",
            "  File \"/tmp/ipython-input-2840071918.py\", line 123, in main\n",
            "    upload_to_bigquery(\n",
            "  File \"/tmp/ipython-input-2380194694.py\", line 30, in upload_to_bigquery\n",
            "    job.result()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/job/base.py\", line 1048, in result\n",
            "    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/future/polling.py\", line 261, in result\n",
            "    raise self._exception\n",
            "google.api_core.exceptions.BadRequest: 400 Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 2; errors: 1. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 2; errors: 1. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: JSON processing encountered too many errors, giving up. Rows: 2; errors: 1; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: JSON parsing error in row starting at position 589: Parser terminated before end of string\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-230879549.py\", line 9, in <cell line: 0>\n",
            "    sys.exit(1)\n",
            "SystemExit: 1\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1701, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-230879549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2840071918.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtable_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset_ref}.{TBL_AGG}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     upload_to_bigquery(\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2380194694.py\u001b[0m in \u001b[0;36mupload_to_bigquery\u001b[0;34m(client, rows, table_id, schema, write_disposition, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_table_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[BQ] Loaded {len(chunk)} rows into {table_id} (batch {i//batch_size+1})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 2; errors: 1. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 2; errors: 1. Please look into the errors[] collection for more details.; reason: invalid, message: Error while reading data, error message: JSON processing encountered too many errors, giving up. Rows: 2; errors: 1; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: JSON parsing error in row starting at position 589: Parser terminated before end of string",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-230879549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nUnexpected error: {e}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m: 1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xA4b9Y_zTfrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}